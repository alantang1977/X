# coding=utf-8
# !/usr/bin/python
import sys
import datetime
from copy import deepcopy
from urllib.parse import urljoin, quote_plus
from lxml import etree
import urllib3

# Suppress SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

sys.path.append('..')
from base.spider import Spider
import json
import re


class Spider(Spider):
    def getName(self):
        return "æ¯’èˆŒç”µå½±"

    def init(self, extend=""):
        print("============æ¯’èˆŒç”µå½± Spider Initialized============")
        print(f"[dushe] Spider loaded successfully!")
        print(f"[dushe] Extend param: {extend}")
        pass

    def homeContent(self, filter):
        result = {}
        cateManual = {
            "ç”µå½±": "1",
            "ç”µè§†å‰§": "2",
            "åŠ¨æ¼«": "3",
            "çºªå½•ç‰‡": "4"
        }

        classes = []
        for k in cateManual:
            classes.append({
                'type_name': k,
                'type_id': cateManual[k]
            })

        result['class'] = classes
        if (filter):
            result['filters'] = self.get_filters()
        return result

    def get_filters(self):
        base = self.config.get('filter', {})
        filt = deepcopy(base)
        current_year = datetime.datetime.now().year
        
        # Build years list: current year to 2013, plus decades
        years = [{"n": "å…¨éƒ¨", "v": ""}]
        for y in range(current_year, 2012, -1):
            years.append({"n": str(y), "v": str(y)})
        years.extend([
            {"n": "90å¹´ä»£", "v": "90å¹´ä»£"},
            {"n": "80å¹´ä»£", "v": "80å¹´ä»£"},
            {"n": "70å¹´ä»£", "v": "70å¹´ä»£"}
        ])
        
        for tid, arr in filt.items():
            for item in arr:
                key = item.get('key') or item.get('k')
                if key in ('year', '11'):
                    if 'value' in item:
                        item['value'] = years
                    else:
                        item['v'] = years
        return filt

    def homeVideoContent(self):
        # Not implemented for this site
        result = {'list': []}
        return result

    def categoryContent(self, tid, pg, filter, extend):
        result = {}
        try:
            ext = extend or {}
            # Extract filter values
            cate_type = ext.get('class') or ext.get('3') or ''
            area = ext.get('area') or ext.get('1') or ''
            year = ext.get('year') or ext.get('11') or ''
            by = ext.get('by') or ext.get('2') or ''
            
            # URL encode Chinese characters
            enc_type = quote_plus(cate_type) if cate_type else ''
            enc_area = quote_plus(area) if area else ''
            enc_year = quote_plus(year) if year else ''
            
            # Map sorting values
            by_map = {
                'æœ€æ–°': '2',
                'æœ€çƒ­': '3',
                'è¯„åˆ†': '4'
            }
            by_val = by_map.get(by, '2')  # Default to æœ€æ–°
            
            # Build URL: /show/{tid}-{type}-{area}--{year}-{by}-{pg}.html
            url = f'https://www.dushe06.com/show/{tid}-{enc_type}-{enc_area}--{enc_year}-{by_val}-{pg}.html'
        except Exception:
            # Fallback URL
            url = f'https://www.dushe06.com/show/{tid}-----2-{pg}.html'

        print(url)
        rsp = self.fetch(url, headers=self.header)
        if not rsp or not rsp.text:
            return result
        
        # Fix encoding - use lxml directly to avoid base class encoding problems
        try:
            if hasattr(rsp, 'content'):
                parser = etree.HTMLParser(encoding='utf-8', recover=True, remove_blank_text=True)
                root = etree.HTML(rsp.content, parser=parser)
            else:
                parser = etree.HTMLParser(encoding='utf-8', recover=True, remove_blank_text=True)
                root = etree.HTML(rsp.text.encode('utf-8', errors='ignore'), parser=parser)
        except Exception:
            root = self.html(rsp.text)
        videos = []
        
        try:
            # Extract video items: <a href="/detail/xxx.html" class="v-item">
            items = root.xpath("//a[@class='v-item']")
            for item in items:
                try:
                    # Extract href
                    hrefs = item.xpath('./@href')
                    if not hrefs:
                        continue
                    href = hrefs[0]
                    
                    # Extract ID from /detail/54125.html
                    m = re.search(r'/detail/(\d+)\.html', href)
                    if not m:
                        continue
                    sid = m.group(1)
                    
                    # Extract title from nested div.v-item-title (not display:none)
                    title_nodes = item.xpath('.//div[@class="v-item-title" and not(contains(@style,"display: none"))]/text()')
                    title = title_nodes[0].strip() if title_nodes else ''
                    
                    # Extract image - second img tag with data-original or src
                    # Skip placeholder images
                    img_nodes = item.xpath('.//img[@class="lazy lazyload"]')
                    pic = ''
                    for img in img_nodes:
                        # Try data-original first
                        src = img.xpath('./@data-original')
                        if not src:
                            src = img.xpath('./@src')
                        if src:
                            img_url = src[0]
                            # Skip placeholder
                            if 'logo_placeholder' not in img_url:
                                # Handle relative URLs
                                if img_url.startswith('/'):
                                    pic = 'https://vres.mgdnka.cn' + img_url
                                elif img_url.startswith('http'):
                                    pic = img_url
                                break
                    
                    # Extract remarks from v-item-bottom span
                    remark_nodes = item.xpath('.//div[@class="v-item-bottom"]//span/text()')
                    remark = remark_nodes[0].strip() if remark_nodes else ''
                    
                    # Extract rating from v-item-top-left span
                    rating_nodes = item.xpath('.//div[@class="v-item-top-left"]//span/text()')
                    if rating_nodes:
                        remark = rating_nodes[0].strip() + ' ' + remark if remark else rating_nodes[0].strip()
                    
                    videos.append({
                        "vod_id": sid,
                        "vod_name": title,
                        "vod_pic": pic,
                        "vod_remarks": remark.strip()
                    })
                except Exception:
                    continue
        except Exception:
            pass
        
        result['list'] = videos
        result['page'] = pg
        result['pagecount'] = 9999
        result['limit'] = 90
        result['total'] = 999999
        return result

    def _create_fallback_vod(self, tid, error_msg):
        """Create a fallback VOD entry when extraction fails"""
        return {
            'list': [{
                'vod_id': tid,
                'vod_name': f'åŠ è½½å¤±è´¥: {error_msg}',
                'vod_pic': '',
                'vod_content': f'æ— æ³•åŠ è½½è§†é¢‘è¯¦æƒ…ã€‚é”™è¯¯: {error_msg}',
                'vod_play_from': 'æ¯’èˆŒç”µå½±',
                'vod_play_url': f'é”™è¯¯${f"https://www.dushe06.com/detail/{tid}.html"}'
            }]
        }
    
    def detailContent(self, ids):
        print(f'[dushe] ===== detailContent called =====')
        print(f'[dushe] ids parameter: {ids}')
        print(f'[dushe] ids type: {type(ids)}')
        
        try:
            if not ids:
                print(f'[dushe] ERROR: ids is empty')
                return {'list': []}
            
            tid = str(ids[0]).strip()
            
            # Check if this is a debug entry - return the stored debug info
            if tid.startswith('debug_'):
                print(f'[dushe] Debug entry clicked')
                # Return a generic debug message since we can't pass the actual debug_info through
                # The real debug info should be visible in the search results vod_content
                return {
                    'list': [{
                        'vod_id': tid,
                        'vod_name': 'ğŸ” æœç´¢è°ƒè¯•ä¿¡æ¯',
                        'vod_pic': 'https://via.placeholder.com/300x400/FF0000/FFFFFF?text=DEBUG',
                        'vod_content': 'âš ï¸ è°ƒè¯•ä¿¡æ¯åº”è¯¥å·²ç»æ˜¾ç¤ºåœ¨æœç´¢ç»“æœé¡µé¢\n\nå¦‚æœæ‚¨çœ‹åˆ°è¿™ä¸ªé¡µé¢ï¼Œè¯´æ˜:\n1. æœç´¢åŠŸèƒ½é‡åˆ°äº†é—®é¢˜\n2. è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯åº”è¯¥åœ¨ä¸Šä¸€é¡µæ˜¾ç¤º\n\nè¯·è¿”å›æœç´¢ç»“æœé¡µæŸ¥çœ‹å®Œæ•´çš„è°ƒè¯•ä¿¡æ¯ï¼ŒåŒ…æ‹¬:\nâ€¢ å¤±è´¥çš„å…·ä½“æ­¥éª¤\nâ€¢ é”™è¯¯ä¿¡æ¯\nâ€¢ TokençŠ¶æ€\nâ€¢ å“åº”é•¿åº¦\nâ€¢ æ‰¾åˆ°çš„é¡¹ç›®æ•°\nâ€¢ æå–çš„è§†é¢‘æ•°\n\nå¦‚æœéœ€è¦æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹FongMiåº”ç”¨æ—¥å¿—ã€‚',
                        'vod_play_from': 'æç¤º',
                        'vod_play_url': 'è¿”å›æœç´¢é¡µ$https://www.dushe06.com'
                    }]
                }
            
            url = f'https://www.dushe06.com/detail/{tid}.html'
            print(f'[dushe] Fetching URL: {url}')
            
            try:
                rsp = self.fetch(url, headers=self.header)
                if not rsp or not rsp.text:
                    print(f'[dushe] ERROR: Failed to fetch detail page')
                    return self._create_fallback_vod(tid, 'è·å–é¡µé¢å¤±è´¥')
                
                print(f'[dushe] Fetch successful, content length: {len(rsp.text)}')
                
                # Fix encoding issues - use lxml directly to avoid base class encoding problems
                try:
                    # Get raw bytes and parse directly with lxml
                    if hasattr(rsp, 'content'):
                        # Parse bytes directly - lxml handles encoding better
                        parser = etree.HTMLParser(encoding='utf-8', recover=True, remove_blank_text=True)
                        root = etree.HTML(rsp.content, parser=parser)
                        print(f'[dushe] HTML parsed with lxml directly from bytes')
                    else:
                        # Fallback to text if content not available
                        html_text = rsp.text.encode('utf-8', errors='ignore').decode('utf-8', errors='ignore')
                        parser = etree.HTMLParser(encoding='utf-8', recover=True, remove_blank_text=True)
                        root = etree.HTML(html_text, parser=parser)
                        print(f'[dushe] HTML parsed with lxml from text')
                except Exception as decode_err:
                    print(f'[dushe] lxml parsing error: {str(decode_err)}')
                    # Last resort: try base class method
                    try:
                        root = self.html(rsp.text)
                        print(f'[dushe] Fallback to self.html() succeeded')
                    except Exception as fallback_err:
                        print(f'[dushe] Fallback also failed: {str(fallback_err)}')
                        root = None
                
                if root is None:
                    print(f'[dushe] ERROR: Failed to parse HTML')
                    return self._create_fallback_vod(tid, 'è§£æHTMLå¤±è´¥')
                
                print(f'[dushe] HTML parsed successfully')
            except Exception as e:
                print(f'[dushe] ERROR in fetch/parse: {str(e)}')
                import traceback
                traceback.print_exc()
                return self._create_fallback_vod(tid, f'å¼‚å¸¸: {str(e)}')
        except Exception as e:
            print(f'[dushe] FATAL ERROR in detailContent: {str(e)}')
            import traceback
            traceback.print_exc()
            return {'list': []}
        
        # Title - from div.detail-title strong (not display:none)
        try:
            title_nodes = root.xpath('//div[@class="detail-title"]/strong[not(contains(text(),"kekys") or contains(text(),"ğ•œğ•œğ•ªğ•¤"))]/text()')
            title = title_nodes[0].strip() if title_nodes else ""
        except Exception:
            title = ""
        
        # Picture
        pic = ""
        try:
            pics = root.xpath('//div[@class="detail-pic"]//img/@src')
            if not pics:
                pics = root.xpath('//div[@class="detail-pic"]//img/@data-original')
            if pics:
                pic = pics[0]
                if pic.startswith('/'):
                    pic = 'https://vres.mgdnka.cn' + pic
        except Exception:
            pic = ""
        
        # Description
        detail = ""
        try:
            detail_nodes = root.xpath('//div[@class="detail-desc"]//p/text()')
            if detail_nodes:
                detail = '\n'.join([d.strip() for d in detail_nodes if d.strip()])
        except Exception:
            detail = ""
        
        # Score - fromè±†ç“£ tag
        douban = ""
        try:
            score_nodes = root.xpath('//div[@class="detail-tags"]//a[contains(@href,"show")]/text()')
            for s in score_nodes:
                if 'åˆ†' not in s:
                    continue
                # Extract number from text like "9.4åˆ†" or just year
                if re.match(r'^\d{4}$', s.strip()):
                    continue
                douban = s.strip()
                break
        except Exception:
            pass
        
        # Remarks
        remarks = ""
        try:
            remark_nodes = root.xpath('//div[@class="detail-info-row"][contains(.//div,"å¤‡æ³¨")]//div[@class="detail-info-row-main"]/text()')
            if remark_nodes:
                remarks = remark_nodes[0].strip()
        except Exception:
            pass
        
        # Director
        director = ""
        try:
            dir_nodes = root.xpath('//div[@class="detail-info-row"][contains(.//div,"å¯¼æ¼”")]//div[@class="detail-info-row-main"]//a/text()')
            if dir_nodes:
                director = '/'.join([d.strip() for d in dir_nodes])
        except Exception:
            pass
        
        # Actor
        actor = ""
        try:
            actor_nodes = root.xpath('//div[@class="detail-info-row"][contains(.//div,"æ¼”å‘˜")]//div[@class="detail-info-row-main"]//a/text()')
            if actor_nodes:
                actor = '/'.join([a.strip() for a in actor_nodes])
        except Exception:
            pass
        
        # Year
        year = ""
        try:
            year_nodes = root.xpath('//div[@class="detail-info-row"][contains(.//div,"é¦–æ˜ ")]//div[@class="detail-info-row-main"]/text()')
            if year_nodes:
                year = year_nodes[0].strip()
        except Exception:
            pass
        
        # Type/Genre from tags
        type_name = ""
        try:
            tag_nodes = root.xpath('//div[@class="detail-tags"]//a[@class="detail-tags-item"]/text()')
            if tag_nodes:
                # Filter out year
                tags = [t.strip() for t in tag_nodes if not re.match(r'^\d{4}$', t.strip())]
                type_name = '/'.join(tags)
        except Exception:
            pass
        
        vod = {
            "vod_id": tid,
            "vod_name": title,
            "vod_pic": pic,
            "type_name": type_name,
            "vod_year": year,
            "vod_area": "",
            "vod_remarks": remarks,
            "vod_actor": actor,
            "vod_director": director,
            "vod_douban_score": douban,
            "vod_content": detail
        }
        
        # Extract play sources and episodes
        playFrom = []
        playList = []
        
        try:
            # Find all source tabs: div.swiper-slide.source-swiper-slide
            source_tabs = root.xpath('//div[contains(@class,"source-swiper-slide")]//a[@class="source-item"]')
            # Find all episode lists: div.episode-list
            episode_lists = root.xpath('//div[@class="episode-list"]')
            
            print(f'[dushe] Found {len(source_tabs)} source tabs, {len(episode_lists)} episode lists')
            
            # Match sources with episode lists
            for idx, source_tab in enumerate(source_tabs):
                try:
                    # Extract source name from span.source-item-label
                    source_name_nodes = source_tab.xpath('.//span[@class="source-item-label"]/text()')
                    source_name = source_name_nodes[0].strip() if source_name_nodes else f"çº¿è·¯{idx+1}"
                    
                    # Get corresponding episode list
                    if idx < len(episode_lists):
                        episode_list = episode_lists[idx]
                        # Extract all episode links
                        episode_links = episode_list.xpath('.//a[@class="episode-item"]')
                        vodItems = []
                        
                        for ep_link in episode_links:
                            try:
                                ep_href = ep_link.xpath('./@href')
                                ep_name = ep_link.xpath('./text()')
                                
                                if ep_href and ep_name:
                                    href = ep_href[0]
                                    name = ep_name[0].strip()
                                    
                                    # Extract play ID from /play/54125-32-25552.html
                                    m = re.search(r'/play/([^.]+)\.html', href)
                                    if m:
                                        play_id = m.group(1)
                                        vodItems.append(f"{name}${play_id}")
                            except Exception as e:
                                print(f'[dushe] Error extracting episode: {str(e)}')
                                continue
                        
                        if vodItems:
                            playFrom.append(source_name)
                            playList.append('#'.join(vodItems))
                            print(f'[dushe] Source {idx+1} "{source_name}": {len(vodItems)} episodes')
                except Exception as e:
                    print(f'[dushe] Error processing source {idx}: {str(e)}')
                    continue
        except Exception as e:
            print(f'[dushe] Error in play extraction: {str(e)}')
        
        # Ensure we have play data
        if not playFrom or not playList:
            print(f'[dushe] WARNING: No play data extracted, using fallback')
            playFrom = ['æ¯’èˆŒç”µå½±']
            playList = [f'æš‚æ— æ’­æ”¾æº$https://www.dushe06.com/detail/{tid}.html']
        
        vod['vod_play_from'] = '$$$'.join(playFrom)
        vod['vod_play_url'] = '$$$'.join(playList)
        
        print(f'[dushe] detailContent result:')
        print(f'  - Title: {vod["vod_name"]}')
        print(f'  - Sources: {len(playFrom)}')
        print(f'  - vod_play_from: {vod["vod_play_from"][:100]}')
        print(f'  - vod_play_url length: {len(vod["vod_play_url"])}')
        
        result = {
            'list': [vod]
        }
        return result

    def searchContent(self, key, quick, pg='1'):
        # Call the actual search implementation
        print(f'[dushe] searchContent called: key={key}, quick={quick}, pg={pg}')
        return self.searchContentPage(key, quick, pg)
    
    def searchContentPage(self, key, quick, pg='1'):
        # Search requires a token from the homepage
        print(f'[dushe] ===== searchContentPage called =====')
        print(f'[dushe] key: {key}, pg: {pg}')
        
        debug_info = {
            'step': 'init',
            'error': None,
            'token': None,
            'url': None,
            'response_length': 0,
            'items_found': 0,
            'videos_extracted': 0
        }
        
        try:
            # Use requests directly instead of self.fetch() to avoid base class interference
            import requests
            import time
            
            # Get homepage to extract search token
            debug_info['step'] = 'fetching_homepage'
            print(f'[dushe] Fetching homepage for token...')
            
            # Add delay before homepage fetch
            time.sleep(0.5)
            
            try:
                home_rsp = requests.get('https://www.dushe06.com', headers=self.header, timeout=15, verify=False)
            except Exception as fetch_err:
                debug_info['error'] = f'Homepage fetch error: {type(fetch_err).__name__}'
                print(f'[dushe] HOMEPAGE FETCH ERROR: {str(fetch_err)}')
                return self._create_debug_result(key, debug_info)
            
            if not home_rsp or not home_rsp.text:
                debug_info['error'] = 'Empty homepage response'
                print(f'[dushe] ERROR: {debug_info["error"]}')
                return self._create_debug_result(key, debug_info)
            
            # Extract token from search form
            debug_info['step'] = 'extracting_token'
            token = ''
            m = re.search(r'name="t"\s+value="([^"]+)"', home_rsp.text)
            if m:
                token = m.group(1)
                debug_info['token'] = token[:20] + '...' if len(token) > 20 else token
                print(f'[dushe] Token found: {token}')
            else:
                debug_info['error'] = 'Token not found in homepage'
                print(f'[dushe] WARNING: Token not found, proceeding without token')
            
            # Build search URL with token
            debug_info['step'] = 'building_url'
            url = f'https://www.dushe06.com/search?t={token}&k={quote_plus(key)}'
            if pg and pg != '1':
                url += f'&page={pg}'
            
            debug_info['url'] = url
            print(f'[dushe] Search URL: {url}')
            
            debug_info['step'] = 'fetching_search'
            
            # Add delay between requests
            time.sleep(0.8)
            
            # Update Referer to homepage for search request
            search_headers = self.header.copy()
            search_headers['Referer'] = 'https://www.dushe06.com/'
            
            try:
                rsp = requests.get(url, headers=search_headers, timeout=15, verify=False)
            except Exception as fetch_err:
                debug_info['error'] = f'Search fetch error: {type(fetch_err).__name__}'
                print(f'[dushe] SEARCH FETCH ERROR: {str(fetch_err)}')
                return self._create_debug_result(key, debug_info)
            
            if not rsp or not rsp.text:
                debug_info['error'] = 'Empty search response'
                print(f'[dushe] ERROR: {debug_info["error"]}')
                return self._create_debug_result(key, debug_info)
            
            debug_info['response_length'] = len(rsp.text)
            print(f'[dushe] Search response length: {len(rsp.text)}')
            
            # Fix encoding - use lxml directly to avoid base class encoding problems
            debug_info['step'] = 'parsing_html'
            try:
                if hasattr(rsp, 'content'):
                    parser = etree.HTMLParser(encoding='utf-8', recover=True, remove_blank_text=True)
                    root = etree.HTML(rsp.content, parser=parser)
                else:
                    parser = etree.HTMLParser(encoding='utf-8', recover=True, remove_blank_text=True)
                    root = etree.HTML(rsp.text.encode('utf-8', errors='ignore'), parser=parser)
            except Exception as e:
                debug_info['error'] = f'HTML parsing error: {str(e)}'
                print(f'[dushe] lxml parsing error: {str(e)}, trying fallback')
                root = self.html(rsp.text)
            
            videos = []
            
            # Search uses different structure: a.search-result-item
            # Note: class attribute may have whitespace/newlines, so use contains()
            debug_info['step'] = 'extracting_items'
            items = root.xpath("//a[contains(@class, 'search-result-item')]")
            debug_info['items_found'] = len(items)
            print(f'[dushe] Found {len(items)} search result items')
            
            for idx, item in enumerate(items):
                try:
                    print(f'[dushe] Processing item {idx+1}...')
                    hrefs = item.xpath('./@href')
                    print(f'[dushe]   hrefs: {hrefs}')
                    if not hrefs:
                        print(f'[dushe]   SKIP: No href found')
                        continue
                    href = hrefs[0]
                    
                    m = re.search(r'/detail/(\d+)\.html', href)
                    if not m:
                        print(f'[dushe]   SKIP: href does not match pattern: {href}')
                        continue
                    sid = m.group(1)
                    print(f'[dushe]   ID: {sid}')
                    
                    # Title from div.title
                    title_nodes = item.xpath('.//div[@class="title"]/text()')
                    title = title_nodes[0].strip() if title_nodes else ''
                    print(f'[dushe]   Title: {title}')
                    
                    if not title:
                        print(f'[dushe]   SKIP: No title found')
                        continue
                    
                    # Image from search-result-item-pic img
                    # Note: There are 2 img tags - first has real image in data-original, second is placeholder
                    img_nodes = item.xpath('.//div[@class="search-result-item-pic"]//img[@class="lazy lazyload"]')
                    pic = ''
                    for img in img_nodes:
                        # Prioritize data-original over src (src is always placeholder)
                        data_orig = img.xpath('./@data-original')
                        if data_orig:
                            img_url = data_orig[0]
                            # Skip placeholder images
                            if 'logo_placeholder' not in img_url and not img_url.startswith('data:'):
                                if img_url.startswith('/'):
                                    pic = 'https://vres.mgdnka.cn' + img_url
                                elif img_url.startswith('http'):
                                    pic = img_url
                                break
                    
                    # Tags as remarks (year/region/genre)
                    tag_nodes = item.xpath('.//div[@class="tags"]//span/text()')
                    remark = '/'.join([t.strip() for t in tag_nodes if t.strip()]) if tag_nodes else ''
                    
                    video_item = {
                        "vod_id": sid,
                        "vod_name": title,
                        "vod_pic": pic,
                        "vod_remarks": remark
                    }
                    videos.append(video_item)
                    print(f'[dushe]   âœ“ Added: {title}')
                    
                except Exception as e:
                    print(f'[dushe] âœ— Error extracting item {idx}: {str(e)}')
                    import traceback
                    traceback.print_exc()
                    continue
            
            debug_info['videos_extracted'] = len(videos)
            debug_info['step'] = 'completed'
            print(f'[dushe] Total extracted: {len(videos)} videos')
            
            # If no videos extracted, return debug info
            if len(videos) == 0:
                debug_info['error'] = 'No videos extracted from items'
                return self._create_debug_result(key, debug_info)
            
            return {'list': videos}
        except Exception as e:
            debug_info['step'] = 'exception'
            debug_info['error'] = str(e)
            print(f'[dushe] FATAL ERROR in searchContentPage: {str(e)}')
            import traceback
            traceback.print_exc()
            return self._create_debug_result(key, debug_info)
    
    def _create_debug_result(self, key, debug_info):
        """Create a debug result entry that shows what went wrong"""
        error_msg = debug_info.get('error', 'Unknown error')
        step = debug_info.get('step', 'unknown')
        
        # Build detailed debug content for vod_content
        debug_content = f"ğŸ” æœç´¢è°ƒè¯•ä¿¡æ¯\n\n"
        debug_content += f"æœç´¢å…³é”®è¯: {key}\n"
        debug_content += f"å¤±è´¥æ­¥éª¤: {step}\n"
        debug_content += f"é”™è¯¯ä¿¡æ¯: {error_msg}\n\n"
        debug_content += f"=== è¯¦ç»†ä¿¡æ¯ ===\n"
        if debug_info.get('token'):
            debug_content += f"Token: {debug_info['token']}\n"
        if debug_info.get('url'):
            debug_content += f"URL: {debug_info['url']}\n"
        debug_content += f"å“åº”é•¿åº¦: {debug_info.get('response_length', 0)} bytes\n"
        debug_content += f"æ‰¾åˆ°é¡¹ç›®æ•°: {debug_info.get('items_found', 0)}\n"
        debug_content += f"æå–è§†é¢‘æ•°: {debug_info.get('videos_extracted', 0)}\n\n"
        debug_content += f"=== å¯èƒ½åŸå›  ===\n"
        if step == 'fetching_homepage':
            debug_content += "â€¢ æ— æ³•è¿æ¥åˆ°ç½‘ç«™é¦–é¡µ\n"
            debug_content += "â€¢ ç½‘ç»œè¿æ¥é—®é¢˜\n"
            debug_content += "â€¢ SSLè¯ä¹¦éªŒè¯å¤±è´¥\n"
        elif step == 'extracting_token':
            debug_content += "â€¢ ç½‘ç«™HTMLç»“æ„å·²æ”¹å˜\n"
            debug_content += "â€¢ Tokenå­—æ®µåç§°å·²æ›´æ–°\n"
        elif step == 'fetching_search':
            debug_content += "â€¢ æœç´¢è¯·æ±‚å¤±è´¥\n"
            debug_content += "â€¢ Tokenå¯èƒ½å·²è¿‡æœŸ\n"
        elif step == 'parsing_html':
            debug_content += "â€¢ HTMLè§£æå¤±è´¥\n"
            debug_content += "â€¢ ç¼–ç é—®é¢˜\n"
        elif step == 'extracting_items':
            debug_content += "â€¢ XPathé€‰æ‹©å™¨ä¸åŒ¹é…\n"
            debug_content += "â€¢ ç½‘ç«™HTMLç»“æ„å·²æ”¹å˜\n"
        elif step == 'completed':
            debug_content += "â€¢ æ‰¾åˆ°äº†é¡¹ç›®ä½†æ— æ³•æå–æ•°æ®\n"
            debug_content += "â€¢ æ ‡é¢˜æˆ–IDå­—æ®µä¸ºç©º\n"
            debug_content += "â€¢ æ£€æŸ¥æ—¥å¿—æŸ¥çœ‹è·³è¿‡åŸå› \n"
        
        debug_content += f"\nè¯·æŸ¥çœ‹FongMiæ—¥å¿—è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯"
        
        # Create a compact remarks string with key info
        remarks = f"æ­¥éª¤:{step} | "
        remarks += f"é¡¹ç›®:{debug_info.get('items_found', 0)} | "
        remarks += f"æå–:{debug_info.get('videos_extracted', 0)}"
        
        print(f'[dushe] Creating debug result for step: {step}')
        print(f'[dushe] Debug info: {debug_info}')
        
        return {
            'list': [{
                'vod_id': 'debug_0',
                'vod_name': f'âŒ æœç´¢å¤±è´¥: {key}',
                'vod_pic': 'https://via.placeholder.com/300x400/FF0000/FFFFFF?text=DEBUG',
                'vod_remarks': remarks,
                'vod_content': debug_content,
                'vod_play_from': 'è°ƒè¯•ä¿¡æ¯',
                'vod_play_url': 'æŸ¥çœ‹ä¸Šæ–¹è¯¦æƒ…$https://www.dushe06.com'
            }]
        }

    def playerContent(self, flag, id, vipFlags):
        """
        Return play page URL with parse=1 to let TVBox resolve the playable URL
        """
        url = f'https://www.dushe06.com/play/{id}.html'
        result = {
            "parse": 1,
            "playUrl": "",
            "url": url,
            "header": self.header
        }
        return result

    def isVideoFormat(self, url):
        pass

    def manualVideoCheck(self):
        pass

    def localProxy(self, param):
        action = {
            'url': '',
            'header': '',
            'param': '',
            'type': 'string',
            'after': ''
        }
        return [200, "video/MP2T", action, ""]

    config = {
        "filter": {
            "1": [  # ç”µå½±
                {
                    "key": "class",
                    "name": "ç±»å‹",
                    "value": [
                        {"n": "å…¨éƒ¨", "v": ""},
                        {"n": "ææ€–", "v": "ææ€–"},
                        {"n": "æƒŠæ‚š", "v": "æƒŠæ‚š"},
                        {"n": "çˆ±æƒ…", "v": "çˆ±æƒ…"},
                        {"n": "åŒæ€§", "v": "åŒæ€§"},
                        {"n": "å–œå‰§", "v": "å–œå‰§"},
                        {"n": "åŠ¨ç”»", "v": "åŠ¨ç”»"},
                        {"n": "çŸ­ç‰‡", "v": "çŸ­ç‰‡"}
                    ]
                },
                {
                    "key": "area",
                    "name": "åœ°åŒº",
                    "value": [
                        {"n": "å…¨éƒ¨", "v": ""},
                        {"n": "æ—¥æœ¬", "v": "æ—¥æœ¬"},
                        {"n": "éŸ©å›½", "v": "éŸ©å›½"},
                        {"n": "ç¾å›½", "v": "ç¾å›½"},
                        {"n": "è‹±å›½", "v": "è‹±å›½"},
                        {"n": "æ³•å›½", "v": "æ³•å›½"},
                        {"n": "å¾·å›½", "v": "å¾·å›½"},
                        {"n": "æ„å¤§åˆ©", "v": "æ„å¤§åˆ©"},
                        {"n": "å·´è¥¿", "v": "å·´è¥¿"},
                        {"n": "ç‘å…¸", "v": "ç‘å…¸"}
                    ]
                },
                {"key": "year", "name": "å¹´ä»½", "value": []},
                {
                    "key": "by",
                    "name": "æ’åº",
                    "value": [
                        {"n": "æœ€æ–°", "v": "æœ€æ–°"},
                        {"n": "æœ€çƒ­", "v": "æœ€çƒ­"},
                        {"n": "è¯„åˆ†", "v": "è¯„åˆ†"}
                    ]
                }
            ],
            "2": [  # ç”µè§†å‰§
                {
                    "key": "class",
                    "name": "ç±»å‹",
                    "value": [
                        {"n": "å…¨éƒ¨", "v": ""},
                        {"n": "ææ€–", "v": "ææ€–"},
                        {"n": "æƒŠæ‚š", "v": "æƒŠæ‚š"},
                        {"n": "çˆ±æƒ…", "v": "çˆ±æƒ…"},
                        {"n": "åŒæ€§", "v": "åŒæ€§"},
                        {"n": "å–œå‰§", "v": "å–œå‰§"},
                        {"n": "åŠ¨ç”»", "v": "åŠ¨ç”»"}
                    ]
                },
                {
                    "key": "area",
                    "name": "åœ°åŒº",
                    "value": [
                        {"n": "å…¨éƒ¨", "v": ""},
                        {"n": "æ—¥æœ¬", "v": "æ—¥æœ¬"},
                        {"n": "éŸ©å›½", "v": "éŸ©å›½"},
                        {"n": "ç¾å›½", "v": "ç¾å›½"},
                        {"n": "è‹±å›½", "v": "è‹±å›½"},
                        {"n": "æ³•å›½", "v": "æ³•å›½"},
                        {"n": "å¾·å›½", "v": "å¾·å›½"},
                        {"n": "æ„å¤§åˆ©", "v": "æ„å¤§åˆ©"},
                        {"n": "å·´è¥¿", "v": "å·´è¥¿"},
                        {"n": "ç‘å…¸", "v": "ç‘å…¸"}
                    ]
                },
                {"key": "year", "name": "å¹´ä»½", "value": []},
                {
                    "key": "by",
                    "name": "æ’åº",
                    "value": [
                        {"n": "æœ€æ–°", "v": "æœ€æ–°"},
                        {"n": "æœ€çƒ­", "v": "æœ€çƒ­"},
                        {"n": "è¯„åˆ†", "v": "è¯„åˆ†"}
                    ]
                }
            ],
            "3": [  # åŠ¨æ¼«
                {
                    "key": "class",
                    "name": "ç±»å‹",
                    "value": [
                        {"n": "å…¨éƒ¨", "v": ""},
                        {"n": "åŒæ€§", "v": "åŒæ€§"},
                        {"n": "ææ€–", "v": "ææ€–"},
                        {"n": "æç¬‘", "v": "æç¬‘"}
                    ]
                },
                {
                    "key": "area",
                    "name": "åœ°åŒº",
                    "value": [
                        {"n": "å…¨éƒ¨", "v": ""},
                        {"n": "æ—¥æœ¬", "v": "æ—¥æœ¬"},
                        {"n": "éŸ©å›½", "v": "éŸ©å›½"},
                        {"n": "ç¾å›½", "v": "ç¾å›½"},
                        {"n": "å…¶ä»–", "v": "å…¶ä»–"}
                    ]
                },
                {"key": "year", "name": "å¹´ä»½", "value": []},
                {
                    "key": "by",
                    "name": "æ’åº",
                    "value": [
                        {"n": "æœ€æ–°", "v": "æœ€æ–°"},
                        {"n": "æœ€çƒ­", "v": "æœ€çƒ­"},
                        {"n": "è¯„åˆ†", "v": "è¯„åˆ†"}
                    ]
                }
            ],
            "4": [  # çºªå½•ç‰‡
                {
                    "key": "class",
                    "name": "ç±»å‹",
                    "value": [
                        {"n": "å…¨éƒ¨", "v": ""},
                        {"n": "åŒæ€§", "v": "åŒæ€§"},
                        {"n": "è®°å½•", "v": "è®°å½•"},
                        {"n": "å†’é™©", "v": "å†’é™©"}
                    ]
                },
                {
                    "key": "area",
                    "name": "åœ°åŒº",
                    "value": [
                        {"n": "å…¨éƒ¨", "v": ""},
                        {"n": "ç¾å›½", "v": "ç¾å›½"},
                        {"n": "è‹±å›½", "v": "è‹±å›½"},
                        {"n": "æ—¥æœ¬", "v": "æ—¥æœ¬"},
                        {"n": "éŸ©å›½", "v": "éŸ©å›½"},
                        {"n": "æ³•å›½", "v": "æ³•å›½"}
                    ]
                },
                {"key": "year", "name": "å¹´ä»½", "value": []},
                {
                    "key": "by",
                    "name": "æ’åº",
                    "value": [
                        {"n": "æœ€æ–°", "v": "æœ€æ–°"},
                        {"n": "æœ€çƒ­", "v": "æœ€çƒ­"},
                        {"n": "è¯„åˆ†", "v": "è¯„åˆ†"}
                    ]
                }
            ]
        }
    }

    header = {
        "Referer": "https://www.dushe06.com",
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-User": "?1",
        "sec-ch-ua": '"Chromium";v="142", "Google Chrome";v="142", "Not_A Brand";v="99"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"macOS"',
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Connection": "keep-alive"
    }
